from concurrent.futures import ThreadPoolExecutor, as_completed
from confluent_kafka import SerializingProducer
from confluent_kafka.schema_registry import SchemaRegistryClient
from confluent_kafka.schema_registry.avro import AvroSerializer
from confluent_kafka.serialization import StringSerializer
import pandas as pd
from datetime import datetime
import time
import os

def update_latency_to_bigquery(latency):

    # Cáº­p nháº­t Ä‘á»™ trá»… vÃ o BigQuery
    # Giáº£ sá»­ báº¡n Ä‘Ã£ cÃ³ má»™t hÃ m Ä‘á»ƒ cáº­p nháº­t Ä‘á»™ trá»… vÃ o BigQuery
    pass


def delivery_report(err, msg):
    if err is not None:
        print(f"âŒ Delivery failed: {err}")
    else:
        print(f"âœ… Delivered to {msg.topic()} [{msg.partition()}] @ offset {msg.offset()}")

schema_str = """
{
  "type": "record",
  "name": "Transaction",
  "fields": [
    {"name": "id", "type": "int"},
    {"name": "trans_date_trans_time", "type": "string"},
    {"name": "cc_num", "type": "string"},
    {"name": "merchant", "type": "string"},
    {"name": "category", "type": "string"},
    {"name": "amt", "type": "double"},
    {"name": "first", "type": "string"},
    {"name": "last", "type": "string"},
    {"name": "gender", "type": "string"},
    {"name": "street", "type": "string"},
    {"name": "city", "type": "string"},
    {"name": "state", "type": "string"},
    {"name": "zip", "type": "int"},
    {"name": "lat", "type": "double"},
    {"name": "long", "type": "double"},
    {"name": "city_pop", "type": "int"},
    {"name": "job", "type": "string"},
    {"name": "dob", "type": "string"},
    {"name": "trans_num", "type": "string"},
    {"name": "unix_time", "type": "long"},
    {"name": "merch_lat", "type": "double"},
    {"name": "merch_long", "type": "double"},
    {"name": "is_fraud", "type": "int"},
    {"name": "produced_timestamp", "type": "long"}
  ]
}
"""

schema_registry_conf = {'url': 'http://' + os.environ.get('zookeeper_ip', 'localhost') + ':8081'}
schema_registry_client = SchemaRegistryClient(schema_registry_conf)
avro_serializer = AvroSerializer(schema_registry_client, schema_str)

producer_conf = {
        'bootstrap.servers': os.environ.get('broker1_ip', 'localhost')+ ':9092',
    'key.serializer': StringSerializer('utf_8'),
    'value.serializer': avro_serializer,
    'queue.buffering.max.messages': 1000000,
    'queue.buffering.max.kbytes': 2097152,  # 2GB
    'linger.ms': 10,
    'batch.num.messages': 10000
}
producer = SerializingProducer(producer_conf)

# Äá»c toÃ n bá»™ file CSV
csv_path = os.environ.get("csv_path", "default_path") 
df = pd.read_csv(csv_path)

# Gá»­i tá»«ng dÃ²ng vá»›i buffer xá»­ lÃ½ cháº·t cháº½
def send_transaction(transaction):
    try:
        # Chuáº©n hÃ³a dá»¯ liá»‡u cáº§n thiáº¿t
        transaction["trans_date_trans_time"] = datetime.now().isoformat()
        transaction["cc_num"] = str(transaction.get("cc_num", "0000000000000000"))
        transaction["zip"] = int(transaction.get("zip", 0))
        transaction["is_fraud"] = int(transaction.get("is_fraud", 0))
        transaction["produced_timestamp"] = int(time.time() * 1000)

        if transaction is None or any(v is None for v in transaction.values()):
            print(f"âš ï¸ Bá» qua transaction ID {transaction.get('id')} vÃ¬ cÃ³ giÃ¡ trá»‹ null")
            return

        transaction_id = transaction.get("id")

        for attempt in range(5):
            try:
                producer.produce(
                    topic="transactions_input_1",
                    key=str(transaction_id),
                    value=transaction,
                    on_delivery=delivery_report
                )
                producer.poll(0)
                break
            except BufferError:
                time.sleep(0.2)
    except Exception as e:
        print(f"âŒ Lá»—i transaction ID {transaction.get('id')}: {e}")

start = datetime.now()

batch_size = 4
rate_limit = 100 # sá»‘ lÆ°á»£ng transaction tá»‘i Ä‘a má»—i giÃ¢y
send = 0
total_sent = 0
now = time.time()

for i in range(0, len(df), batch_size):
    df_batch = df[i:i + batch_size]

    # Náº¿u gá»­i batch nÃ y mÃ  vÆ°á»£t giá»›i háº¡n 8/s â†’ cáº¯t bá»›t láº¡i cho vá»«a
    if send + len(df_batch) > rate_limit:
        df_batch = df_batch.iloc[:rate_limit - send]

    print(f"ðŸš€ Gá»­i batch {i} âžœ {i + len(df_batch)}...")

    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = [executor.submit(send_transaction, row.to_dict()) for _, row in df_batch.iterrows()]
        for future in as_completed(futures):
            pass

    producer.flush()
    send += len(df_batch)
    total_sent += len(df_batch)
    print(f"âœ… ÄÃ£ gá»­i {total_sent}/{len(df)} transactions")

    # Náº¿u Ä‘Ã£ Ä‘á»§ giá»›i háº¡n 8 transaction/s â†’ chá» Ä‘áº¿n giÃ¢y tiáº¿p theo
    if send >= rate_limit:
        while time.time() - now < 1.0:
            time.sleep(0.01)  # ngá»§ má»™t chÃºt Ä‘á»ƒ khÃ´ng dÃ¹ng CPU nhiá»u
        now = time.time()
        send = 0  # reset láº¡i sá»‘ lÆ°á»£ng Ä‘Ã£ gá»­i trong giÃ¢y má»›i

end = datetime.now()

print("----------------------------------------------------------------------------------------------------------------")
print(f"ðŸŽ‰ ÄÃ£ gá»­i toÃ n bá»™ {total_sent} transaction trong {end - start}")
print("----------------------------------------------------------------------------------------------------------------")

